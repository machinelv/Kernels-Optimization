{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import multiprocessing\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional\n",
    "import yaml\n",
    "\n",
    "import torch.cuda\n",
    "\n",
    "from utils import set_seed\n",
    "try:\n",
    "    from task import TestSpec\n",
    "except ImportError:\n",
    "    TestSpec = dict\n",
    "\n",
    "from reference import check_implementation, generate_input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class TestCase:\n",
    "    args: dict\n",
    "    spec: str\n",
    "    memory_usage: Optional[float] = None\n",
    "    FLOPs: Optional[int] = None\n",
    "\n",
    "def _combine(a: int, b: int) -> int:\n",
    "    # combine two integers into one:\n",
    "    # we need this to generate a secret seed based on the test-level seed and\n",
    "    # the global secret seed.\n",
    "    # the test-level seeds are public knowledge, and typically relatively small numbers,\n",
    "    # so we need to make sure they don't provide any useful info for the full seed.\n",
    "    # This Cantor construction ensures that if the secret seed is a large number,\n",
    "    # then so is the overall seed.\n",
    "    return int(a + (a+b)*(a+b+1)//2)\n",
    "\n",
    "\n",
    "def get_test_cases(file_name: str, data_type: str, seed: Optional[int]) -> list[TestCase]:\n",
    "    try:\n",
    "        with open(file_name, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "    except Exception as E:\n",
    "        print(f\"Could not open test file`{file_name}`: {E}\", file=sys.stderr)\n",
    "        exit(113)\n",
    "\n",
    "    tests_data = data.get(data_type, None)\n",
    "    if tests_data is None:\n",
    "        print(f\"Could not find test data for type `{data_type}` in file `{file_name}`\", file=sys.stderr)\n",
    "        exit(113)\n",
    "\n",
    "    tests = []\n",
    "    for data in tests_data:\n",
    "        memory_usage = (data['m'] * data['n'] + data['k'] * data['n'] + data['k'] * data['m']) * 2 / 1024 / 1024\n",
    "        FLOPs = (data['m'] * data['n'] * data['k']) * 2\n",
    "        tests.append(TestCase(spec=str(data), args=data, memory_usage=memory_usage, FLOPs=FLOPs))\n",
    "\n",
    "    if seed is not None:\n",
    "        for test in tests:\n",
    "            if \"seed\" in test.args:\n",
    "                test.args[\"seed\"] = _combine(test.args[\"seed\"], seed)\n",
    "\n",
    "    return tests\n",
    "\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Stats:\n",
    "    runs: int\n",
    "    mean: float\n",
    "    std: float\n",
    "    err: float\n",
    "    best: float\n",
    "    worst: float\n",
    "\n",
    "def calculate_stats(durations: list[int]):\n",
    "    \"\"\"\n",
    "    Calculate statistical data from a list of durations.\n",
    "\n",
    "    @param durations: A list of durations in nanoseconds.\n",
    "    @return: A Stats object containing the number of runs, mean, standard deviation, error, best, and worst durations.\n",
    "    \"\"\"\n",
    "    runs = len(durations)\n",
    "    total = sum(durations)\n",
    "    best = min(durations)\n",
    "    worst = max(durations)\n",
    "\n",
    "    avg = total / runs\n",
    "    variance = sum(map(lambda x: (x - avg)**2, durations))\n",
    "    std = math.sqrt(variance / (runs - 1))\n",
    "    err = std / math.sqrt(runs)\n",
    "\n",
    "    return Stats(runs=runs, mean=avg, std=std, err=err, best=float(best),\n",
    "                 worst=float(worst))\n",
    "\n",
    "def _clone_data(data):\n",
    "    \"\"\"\n",
    "    Recursively goes through data and clones all tensors.\n",
    "    \"\"\"\n",
    "    if isinstance(data, tuple):\n",
    "        return tuple(_clone_data(x) for x in data)\n",
    "    elif isinstance(data, list):\n",
    "        return [_clone_data(x) for x in data]\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: _clone_data(v) for k, v in data.items()}\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.clone()\n",
    "    else:\n",
    "        return data\n",
    "        \n",
    "def wrap_check_implementation(data, submission_output):\n",
    "    # Old version returned just a single string, new version\n",
    "    # returns (bool, str); this function ensures compatibility with old\n",
    "    # problem definitions.\n",
    "    result = check_implementation(data, submission_output)\n",
    "    if isinstance(result, tuple):\n",
    "        return result\n",
    "    else:\n",
    "        return not bool(result), result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff58c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_single_test(test: TestCase):\n",
    "    \"\"\"\n",
    "    Runs a single test case. Do not call directly\n",
    "    \"\"\"\n",
    "    from triton_v0 import custom_kernel\n",
    "\n",
    "    data = generate_input(**test.args)\n",
    "    start_time = time.time()\n",
    "    torch.cuda.synchronize()\n",
    "    submission_output =  custom_kernel(_clone_data(data))\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    duration = float((end_time - start_time) * 1e3)  # convert to nanoseconds\n",
    "    good, message = wrap_check_implementation(data, submission_output)\n",
    "    return good, message, duration\n",
    "\n",
    "def run_single_test(pool: multiprocessing.Pool, test: TestCase):\n",
    "    \"\"\"\n",
    "    Runs a single test in another process.\n",
    "    \"\"\"\n",
    "    return pool.apply(_run_single_test, (test,))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5142562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 42\n",
    "set_seed(seed or 42)\n",
    "tests_data = get_test_cases('./task.yml', 'tests', seed)\n",
    "\n",
    "# import multiprocessing\n",
    "# mp_context = multiprocessing.get_context('spawn')\n",
    "# with mp_context.Pool(1) as pool:\n",
    "#     for idx, test in enumerate(tests_data):\n",
    "#         good, message = run_single_test(pool, test)\n",
    "# import time\n",
    "\n",
    "\n",
    "for idx, test in enumerate(tests_data):\n",
    "    print(f\"test.{idx}.name\", test.spec)\n",
    "    good, message, duration = _run_single_test(test)\n",
    "    if not good:\n",
    "        print(f\"test.{idx}.status\", \"fail\")\n",
    "        print(f\"test.{idx}.error\", message)\n",
    "        passed = False\n",
    "    else:\n",
    "        print(f\"test.{idx}.status\", \"pass\")\n",
    "        print(f\"test.{idx}.duration {duration:.4f}ms\")\n",
    "        print(f\"test.{idx}.TFLOPS {test.FLOPs/duration*1e-9:.4f}\")\n",
    "        if message:\n",
    "            print(f\"test.{idx}.message\", f\"{message}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "a = torch.randn((10, 8), device=DEVICE, dtype=torch.float16)\n",
    "\n",
    "print(a)\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        print(f\"i={i}, j={j}\")\n",
    "        print(a[i*5:(i+1)*5,j*2:(j+1)*2])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "\n",
    "@triton.jit\n",
    "def gather_kernel(src_ptr, index_ptr, out_ptr, stride_m, stride_n, M, N):\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    # 当前行的 offsets\n",
    "    offs_n = tl.arange(0, 4)\n",
    "\n",
    "    # 加载当前行的 index：形状为 [N]\n",
    "    index = tl.load(index_ptr + pid * N + offs_n)  # 每列选哪个行\n",
    "    # src shape: [M, N]\n",
    "    # index shape: [N]\n",
    "    \n",
    "    # 将 src 转为 shape [M, N] 的 tensor\n",
    "    src = tl.load(src_ptr + pid * stride_m + offs_n * stride_n)\n",
    "\n",
    "    # 使用 gather 沿着 axis=0（即行方向）提取\n",
    "    # 每列根据 index 中的值从 src 中选一行\n",
    "    result = tl.gather(src, index, axis=1)\n",
    "\n",
    "    # 存储结果\n",
    "    tl.store(out_ptr + pid * N + offs_n, result)\n",
    "\n",
    "# 模拟输入数据\n",
    "M, N = 4, 4\n",
    "src = torch.arange(0, M * N, dtype=torch.float32, device='cuda').reshape(M, N)\n",
    "index = torch.tensor([[3,1,2,3],\n",
    "                      [0,1,2,3],\n",
    "                      [0,1,2,3],\n",
    "                      [0,1,2,3]], dtype=torch.int32, device='cuda')  # 每行表示每列从哪一行提\n",
    "out = torch.empty((4, N), dtype=torch.float32, device='cuda')\n",
    "\n",
    "# 启动 kernel\n",
    "gather_kernel[(4,)](\n",
    "    src, index, out,\n",
    "    src.stride(0), src.stride(1),\n",
    "    M, N\n",
    ")\n",
    "\n",
    "print(\"src:\\n\", src.cpu())\n",
    "print(\"index:\\n\", index.cpu())\n",
    "print(\"gather result:\\n\", out.cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd-infer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
